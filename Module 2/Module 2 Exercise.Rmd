---
title: "Module 2 Exercise"
author: "Laura Robles-Torres"
date: "2025-06-29"
output:
  html_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=TRUE, warning=FALSE}

library(ggbiplot)
library(tidyverse)
library(dplyr)
library(stats)
library(factoextra)
library(cluster)
```

#### Principal Component Analysis 
### Step 1: Load and Prepare Dataset

```{r load data}
# Read data, assign column names, and display structure
clinic_data = read.csv("./Clinic_GA4.csv", header = TRUE) 

clinic_data = 
  clinic_data |> 
  janitor::clean_names() |>
  mutate(across(everything(), ~ifelse(. == "?", NA, .)),
         age = as.numeric(age),
         glucose = as.numeric(glucose)
        ) # Clean data of '?' and convert 'bare_nuclei' to numeric

clinic_data |>
  filter(if_any(everything(), is.na)) #make sure no missing
```

### Scaling if necessary

```{r scale}
#Obtain and compare means and standard deviations across features. na.rm removes the missings
#We scale for variables to have similar means and SDs

# Calculate column means and standard deviations using tidyverse
clinic_data |> 
  summarise_all(mean, na.rm = TRUE) |> 
  print()

clinic_data |> 
  summarise_all(sd, na.rm = TRUE) %>%
  print()
```

```{r pca}
set.seed(111)

clinic_pca =
clinic_data |>
  na.omit(clinic_data) |> #removes all missing data 
  prcomp(center = TRUE, scale = TRUE) #pca object

clinic_pca$scale #compare the SDs at scale to the SDs not scaled and check that they are similar 

# Generates scree plot
fviz_eig(clinic_pca)
    #you want to explain the most amount of variance with the least amount of PCs. we would like to limit where each component is explaining more than previous component 

#PCA results
clinic_pca |>
  summary() 
```

How many components would you retain in subsequent analyses? What proportion of the total variance do those components explain?

I would probably keep up to four components in this case to explain more than 75% of the variance. 

```{r pca 2}
# Identify how features loaded on the different components
clinic_pca$rotation #you also go off the SD -- if its above 1 then it is explaining more than just that one component

#plot this visually
ggbiplot(clinic_pca) 

ggbiplot(clinic_pca, choices = c(2, 3)) #lets you look at specific PCs (2 and 3) 
```

What do this scree plot and factor loadings tell us about the data? 

On PCA1, homa, insulin and glucose, and leptin are all pretty highly loading. For PCA2, BMI and leptin are more so. For PCA3, triglycerides are negatively loading the mostly. That component is loading having low triglycerides. 

PC1 → high positive loadings on glucose, insulin, HOMA, and leptin, which suggests PC1 captures overall metabolic / glucose-insulin regulation pattern.

PC2 → BMI is positively loading and adiponectin is negatively loading. Maybe reflects an adiposity vs adiponectin axis.

PC3 → Leptin is negatively loading while triglyceride is positively loading. Suggests a lipid/leptin component.

PC4 → Heavy loading on age and this component is likely dominated by age-related effects. 

#### K-means cluster analysis of crime in the US 

The built-in R dataset USArrests includes the crime statistics for each of the 50 US states in 1973. Incidence of arrest, per 100,000 residents for assault, murder and rape are included along with the proportion of the population that lives in urban communities.

Goal: identify clusters of states based on their crime stats using k-means cluster analysis


### Step 1: Load data and prepare for analysis
```{r data load 2}

# Load built-in R dataset
arrests = USArrests

#Is scaling necessary? Lets look at the means and standard deviations of each column. 

# Check means and SDs to determine if scaling is necessary. In general, we always scale. You want similar means and similar SDs. 
arrests |> 
  summarise_all(mean, na.rm = TRUE) %>%
  print()

arrests |> 
  summarise_all(sd, na.rm = TRUE) %>%
  print()

#They do not have similar means or SDs. So we do need to scale! 

scaled_arrests =
  arrests |> 
  mutate(across(where(is.numeric), ~ as.numeric(scale(.))))
```


### Step 2: Conduct a clustering analysis using k-means clustering
We can use the kmeans function in order to identify clusters within the data, based on the three variables.
```{r}
set.seed(100)

#Generate elbow plot to get a sense of what the optimal number of clusters may be. But...where's the elbow? 
fviz_nbclust(scaled_arrests, kmeans, method="wss")
  
#Conduct a gap statistic analysis to determine optimal number of clusters
set.seed(100)
gap_stat = 
  clusGap(scaled_arrests, FUN=kmeans, nstart=25, K.max=7, B=50) |>  #the nstart tells it to do this 25 times and then                pick the clusters out of that so you avoid the first choice of where clusters choice of being       overinfluential. can be 15 or 25 usually. 
  #specified k.max = 7 based on the elbow plot, after 7 clusters, not much variance seen.
  #specified b = 50: numbers of bootstraps to create that null distribution, you want it between 100-200 usually. you want a really stable null distribution to compare your results to
  print(method="firstmax") #first max: the place where the test statistic is the maximum the very first time 

#at k = 4, the gap statistic reaches its maximum = 0.285

output = 
  data.frame(gap_stat$Tab) |>
  print() #store output of gap statistic analysis as a data frame

max.gap = 
  which.max(output$gap) |>
  print()

fviz_gap_stat(gap_stat) #confirms optimal is 4

clusters.max_gap =
  kmeans(scaled_arrests, max.gap, nstart =25) |>
  print() #running the actual object, almost like a summary. It tells you the number of clusters, size of each cluster, the cluster means, etc. Remember this is scaled.

fviz_cluster(clusters.max_gap, data=scaled_arrests)
```

Cluster 1: High rates of all crimes and high urban population.(1.27)
Cluster 2: Low rates of all crimes and low urban population (-0.96)
Cluster 3: Lower rates of all crimes and medium urban population (0.57)
Cluster 4: High rates of all crimes but low urban population (0.0)

#Remember, this are interpreted as scaled numbers, so between -1 and 1. If above 1, it is above average. If below 0 then it is below average. 


#### Exercise 2.3

These data were collected as part of an online survey related to drug and alcohol use and personality traits. Individuals answered standardized questions which were used to calculate continuous scores on personality traits. Individuals were also asked about consumption of alcohol and multiple drugs. Further information on this dataset can be found at http://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29#.

Below is a list of the 7 features and outcome variable within the dataset. In general, the higher value of the score, the greater the personality trait observed within the individual based on the questionnaire.
1.	alc_consumption: CurrentUse, NotCurrentUse 
2.	neurotocism_score:  Measure of Neuroticism
3.	extroversion_score: Measure of Extroversion
4.	openness_score: Measure of Openness to Experiences
5.	agreeableness_score: Measure of Agreeableness
6.	conscientiousness_score: Measure of Conscientiousness
7.	impulsiveness_score: Measure of Impulsivity
8.	sens_seeking_score: Measure of Sensation-Seeking Behaviors.

Goal: You want to predict current alcohol consumption but it is expensive and time-consuming to administer all of the behavioral testing that produces the personality scores. 

*Conduct a reproducible analysis to build and test a classification model using the approach of your choice.*

I decided to utilize the KNN approach given that all variables are continuous with a classification goal.

```{r preprocessing}
set.seed(123)
alcohol_use <- read.csv("Module 2/alcohol_use.csv")

#Look at features
str(alcohol_use)

#drop ID variable
alcohol_use$X<-NULL

#Make outcome category a factor var
alcohol_use_clean =
  alcohol_use |>
  mutate(
    alc_consumption = factor(alc_consumption)) 

#Check distributions, missing data etc.
summary(alcohol_use_clean)

#Omit those with missing data
alcohol_use_clean<-na.omit(alcohol_use_clean)

#Split data 70/30
train.indices =
  alcohol_use_clean$alc_consumption |> 
    createDataPartition(p=0.7, list=F)

train.data =
  alcohol_use_clean |>
  slice(train.indices)

test.data =
  alcohol_use_clean |>
  slice(-train.indices)
```


### Train and assess performance of model

We will use 10-fold cross validation to compare 10 different values of k. We will also use under-sampling due to the imbalance of the data.

```{r models}
set.seed(123)

#Set control options..using 10-fold cross-validation and using under-sampling ("down") due to unbalanced data
trnctrl<-trainControl(method="cv", number=10, sampling="down")

knn.model.1<-train(
                    alc_consumption~.  ,  #all of the data with the outcome of interest outcome.class
                    data=train.data, 
                    method="knn",  
                    trControl=trnctrl, #object that contains the train control function 
                    preProcess=c("center", "scale"), #based on distance 
                    tuneLength=10) #try 10 different k values 

#Identify optimal number of k
knn.model.1$bestTune #result: 23

#See full set of results
knn.model.1$results #accuracy is greatest when k=23

#Create plot of Accuracy vs Choice of K
plot(knn.model.1$results$k, knn.model.1$results$Accuracy, type="l")

confusionMatrix(knn.model.1) 

#REPEAT using over-sampling due to unbalanced data
set.seed(123)
trnctrl<-trainControl(method="cv", number=10, sampling="up")

knn.model.2<-train(
                    alc_consumption~.  , 
                    data=train.data, 
                    method="knn", 
                    trControl=trnctrl, 
                    preProcess=c("center", "scale"), 
                    tuneLength=10)

knn.model.2$bestTune #result is 23 when you over sample 

#Create plot of Accuracy vs Choice of K
plot(knn.model.2$results$k, knn.model.2$results$Accuracy, type="l")

confusionMatrix(knn.model.2) #accuracy is the same with 23

#REPEAT using sequence of k defined by user: you can put in your own k value
set.seed(123)
k.vec<-seq(1,30,1)
knn.model.3<-train(
                    alc_consumption~.  , 
                    data=train.data, 
                    method="knn", 
                    trControl=trnctrl, 
                    preProcess=c("center", "scale"), 
                    tuneGrid=expand.grid(k=k.vec)) #i named my object k.vec above

#Identify optimal number of k
knn.model.3$bestTune #result is 19 when i set k = 30

#See full set of results
knn.model.3$results

plot(knn.model.3$results$k, knn.model.3$results$Accuracy, type="l")

confusionMatrix(knn.model.3) #highest accuracy so far by a bit only although not super different
```

```{r testing_knn}
### Make predictions in test-set using better model

##Note correction in this version. Changed "new_data" to "newdata" to get predictions on test.data
model.results.3 <- predict(knn.model.3, newdata = test.data)

test.outcome <- test.data |>
  dplyr::select(-alc_consumption) |>
  mutate(predicted = predict(knn.model.3, newdata = test.data)) #make predictions, label them as predicted, add that back into the test.data dataset but in this case replace the outcome.class column so we have predicted vs observed

confusionMatrix(test.outcome$predicted, test.data$alc_consumption, positive="CurrentUse")
```


